# ðŸ IPL Data Engineering Pipeline Project

A complete end-to-end real-time data engineering pipeline to stream IPL player statistics from Kafka, process it with Spark, store it in BigQuery, and visualize it on Looker Studio.

---

## ðŸ“Œ Project Overview

This project simulates a real-time data pipeline using IPL playersâ€™ data. It follows a production-grade stack using message queues, stream processing, cloud storage, transformation layers, and dashboarding â€” all automated and scalable.

---

## ðŸ”§ Tech Stack Used

- **Kafka** - Stream data from a Python producer
- **Spark Structured Streaming** - Consume Kafka streams and process
- **Google Cloud Storage (GCS)** - Data lake (CSV landing zone)
- **BigQuery** - Data warehouse
- **dbt (Data Build Tool)** - Transform and model data
- **Looker Studio** - Visualization and dashboard
- **Docker & Docker Compose** - Containerization
- **Python (pandas, faker)** - Data generation
- **GitHub** - Version control and documentation

---

## âš™ï¸ Pipeline Architecture

Kafka Producer â†’ Kafka â†’ Spark Structured 
â†“
Streaming â†’ GCS (CSV)
â†“
â†“â€”â€”â†’ BigQuery via dbt
â†“
Looker Studio

---

## ðŸš€ Pipeline Phases

ðŸ“ All documentation for each phase is available in the [`docs/`](./docs/) folder:

| Phase | Description |
|-------|-------------|
| Phase 1 | Setup Kafka, Producer, and Docker environment |
| Phase 2 | Stream IPL data via Kafka to Spark |
| Phase 3 | Store processed data in GCS |
| Phase 4 | Load GCS data to BigQuery |
| Phase 5 | Data modeling using dbt |
| Phase 6 | Dashboarding using Looker Studio |
| Bonus | Optional Flow with Airflow â†’ PostgreSQL â†’ JDBC â†’ Spark |

---

## ðŸ“‚ Folder Structure

airflow_project/
â”‚
â”œâ”€â”€ kafka/ # Kafka producer setup
â”œâ”€â”€ spark/ # Spark docker & script
â”œâ”€â”€ dbt_venv/ # dbt virtualenv (not pushed)
â”œâ”€â”€ ipl_dbt_project/ # dbt models & configs
â”œâ”€â”€ docs/ # All documentation phase-wise
â”œâ”€â”€ dags/ # Airflow DAGs (optional)
â”œâ”€â”€ logs/ # Spark & Airflow logs
â”œâ”€â”€ jars/ # Spark GCS connectors
â”œâ”€â”€ ipl-streaming-*.json # GCP credentials

---

## ðŸ› ï¸ How to Run This Project Locally

> Make sure Docker, Python 3.8+, and a GCP project are configured

### Step 1: Clone the Repo

```bash
git clone https://github.com/Yash-1115-coder/IPL-Data-Pipeline.git
cd IPL-Data-Pipeline

### Step 2: Start All Services

docker-compose up -d --build

### Step 3: Run Kafka Producer

cd kafka
python3 producer.py

### Step 4: Run Spark Streaming

docker exec -it spark bash
spark-submit --conf spark.hadoop.google.cloud.auth.service.account.enable=true \
             --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/app/ipl-streaming-project-xxxxxx.json \
             spark_stream.py


### Step 5: Setup dbt and Transform Data

cd ipl_dbt_project
dbt run

### Step 6: Connect to Looker Studio and Visualize

Connect BigQuery table ipl_dbt_dataset.stg_ipl_stats to Looker Studio and create charts.

---

## ðŸŒŸ Key Highlights
âœ… Real-time streaming via Kafka & Spark

âœ… GCS integration without cloud SDK on WSL

âœ… BigQuery modeling with dbt

âœ… Fully functional dashboard with filters

âœ… Complete documentation of all phases

---

## ðŸ“š Credits
Course Inspiration: DataTalksClubâ€™s DE Zoomcamp

Built & implemented by Yashas Rajesh

---
