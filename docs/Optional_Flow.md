# ğŸ§ª Optional Flow: Kafka â†’ Airflow DAG â†’ PostgreSQL â†’ Spark (via JDBC)

This document explains how to complete the **JDBC-based Spark streaming from PostgreSQL**, after data has already been ingested from Kafka via Airflow.

---

## âœ… Objective

Use Spark to read the data stored in PostgreSQL using JDBC and proceed with processing.

---

## ğŸ§± Setup Requirements

- PostgreSQL running in Docker (named `airflow_postgres`)
- Table: `ipl_players` already filled via Kafka + Airflow
- Spark container up and running
- PostgreSQL JDBC `.jar` driver added to the Spark container

---

## ğŸ“ Step 1: Ensure PostgreSQL Table Exists

```sql
-- Connect to PostgreSQL inside Docker
docker exec -it airflow_postgres psql -U airflow -d ipl

-- Run this in psql
SELECT * FROM ipl_players LIMIT 5;

## ğŸ› ï¸ Step 2: Place PostgreSQL JDBC .jar in Spark
###Place the driver .jar (e.g. postgresql-42.5.0.jar) in the local jars/ folder

###Ensure your Dockerfile.spark has this line:

dockerfile
COPY ./jars/*.jar /opt/spark/jars/
Rebuild Spark:
docker-compose build spark


##ğŸš€ Step 3: Spark Code to Read from PostgreSQL
Create a Python script (e.g. read_postgres.py) in your /spark/ folder:


from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Postgres to Spark") \
    .config("spark.jars", "/opt/spark/jars/postgresql-42.5.0.jar") \
    .getOrCreate()

jdbc_url = "jdbc:postgresql://airflow_postgres:5432/ipl"
table_name = "ipl_players"
properties = {
    "user": "airflow",
    "password": "airflow",
    "driver": "org.postgresql.Driver"
}

df = spark.read.jdbc(
    url=jdbc_url,
    table=table_name,
    properties=properties
)

df.show()


## ğŸ’» Step 4: Run the Spark Job
### Step inside Spark container
docker exec -it spark bash

### Run the script
spark-submit read_postgres.py
Youâ€™ll see the output from PostgreSQL loaded directly into Spark.

---

##ğŸ§  Why This Matters
This approach gives you:

Structured ingestion via Airflow

Storage in PostgreSQL for audit or backups

Flexibility to read and process in Spark

âœ… Perfect for batch processing or hybrid setups

---

##ğŸ’¬ Summary
1ï¸âƒ£	Kafka - produces data
2ï¸âƒ£	Airflow DAG - reads Kafka & inserts into PostgreSQL
3ï¸âƒ£	Spark - connects to PostgreSQL using JDBC
4ï¸âƒ£	Spark - reads data â†’ transforms or stores it

